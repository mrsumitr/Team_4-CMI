{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43903b6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-13T20:53:56.478865Z",
     "iopub.status.busy": "2025-11-13T20:53:56.478550Z",
     "iopub.status.idle": "2025-11-13T20:53:58.571744Z",
     "shell.execute_reply": "2025-11-13T20:53:58.570204Z"
    },
    "papermill": {
     "duration": 2.101031,
     "end_time": "2025-11-13T20:53:58.573700",
     "exception": false,
     "start_time": "2025-11-13T20:53:56.472669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/cmi_inference_server.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/cmi_gateway.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/__init__.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/templates.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/base_gateway.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/relay.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/kaggle_evaluation.proto\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/__init__.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/kaggle_evaluation_pb2.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/kaggle_evaluation_pb2_grpc.py\n",
      "/kaggle/input/cmi-detect-behavior-with-sensor-data/kaggle_evaluation/core/generated/__init__.py\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a172ba75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:53:58.583562Z",
     "iopub.status.busy": "2025-11-13T20:53:58.583071Z",
     "iopub.status.idle": "2025-11-13T20:53:58.588238Z",
     "shell.execute_reply": "2025-11-13T20:53:58.587230Z"
    },
    "papermill": {
     "duration": 0.012274,
     "end_time": "2025-11-13T20:53:58.590258",
     "exception": false,
     "start_time": "2025-11-13T20:53:58.577984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b371adeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:53:58.599512Z",
     "iopub.status.busy": "2025-11-13T20:53:58.599173Z",
     "iopub.status.idle": "2025-11-13T20:55:19.590833Z",
     "shell.execute_reply": "2025-11-13T20:55:19.589650Z"
    },
    "papermill": {
     "duration": 81.000988,
     "end_time": "2025-11-13T20:55:19.595062",
     "exception": false,
     "start_time": "2025-11-13T20:53:58.594074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(574945, 341) (107, 336)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>sequence_type</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sequence_counter</th>\n",
       "      <th>subject</th>\n",
       "      <th>orientation</th>\n",
       "      <th>behavior</th>\n",
       "      <th>phase</th>\n",
       "      <th>gesture</th>\n",
       "      <th>acc_x</th>\n",
       "      <th>...</th>\n",
       "      <th>tof_5_v54</th>\n",
       "      <th>tof_5_v55</th>\n",
       "      <th>tof_5_v56</th>\n",
       "      <th>tof_5_v57</th>\n",
       "      <th>tof_5_v58</th>\n",
       "      <th>tof_5_v59</th>\n",
       "      <th>tof_5_v60</th>\n",
       "      <th>tof_5_v61</th>\n",
       "      <th>tof_5_v62</th>\n",
       "      <th>tof_5_v63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SEQ_000007_000000</td>\n",
       "      <td>Target</td>\n",
       "      <td>SEQ_000007</td>\n",
       "      <td>0</td>\n",
       "      <td>SUBJ_059520</td>\n",
       "      <td>Seated Lean Non Dom - FACE DOWN</td>\n",
       "      <td>Relaxes and moves hand to target location</td>\n",
       "      <td>Transition</td>\n",
       "      <td>Cheek - pinch skin</td>\n",
       "      <td>6.683594</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SEQ_000007_000001</td>\n",
       "      <td>Target</td>\n",
       "      <td>SEQ_000007</td>\n",
       "      <td>1</td>\n",
       "      <td>SUBJ_059520</td>\n",
       "      <td>Seated Lean Non Dom - FACE DOWN</td>\n",
       "      <td>Relaxes and moves hand to target location</td>\n",
       "      <td>Transition</td>\n",
       "      <td>Cheek - pinch skin</td>\n",
       "      <td>6.949219</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SEQ_000007_000002</td>\n",
       "      <td>Target</td>\n",
       "      <td>SEQ_000007</td>\n",
       "      <td>2</td>\n",
       "      <td>SUBJ_059520</td>\n",
       "      <td>Seated Lean Non Dom - FACE DOWN</td>\n",
       "      <td>Relaxes and moves hand to target location</td>\n",
       "      <td>Transition</td>\n",
       "      <td>Cheek - pinch skin</td>\n",
       "      <td>5.722656</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SEQ_000007_000003</td>\n",
       "      <td>Target</td>\n",
       "      <td>SEQ_000007</td>\n",
       "      <td>3</td>\n",
       "      <td>SUBJ_059520</td>\n",
       "      <td>Seated Lean Non Dom - FACE DOWN</td>\n",
       "      <td>Relaxes and moves hand to target location</td>\n",
       "      <td>Transition</td>\n",
       "      <td>Cheek - pinch skin</td>\n",
       "      <td>6.601562</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEQ_000007_000004</td>\n",
       "      <td>Target</td>\n",
       "      <td>SEQ_000007</td>\n",
       "      <td>4</td>\n",
       "      <td>SUBJ_059520</td>\n",
       "      <td>Seated Lean Non Dom - FACE DOWN</td>\n",
       "      <td>Relaxes and moves hand to target location</td>\n",
       "      <td>Transition</td>\n",
       "      <td>Cheek - pinch skin</td>\n",
       "      <td>5.566406</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 341 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              row_id sequence_type sequence_id  sequence_counter      subject  \\\n",
       "0  SEQ_000007_000000        Target  SEQ_000007                 0  SUBJ_059520   \n",
       "1  SEQ_000007_000001        Target  SEQ_000007                 1  SUBJ_059520   \n",
       "2  SEQ_000007_000002        Target  SEQ_000007                 2  SUBJ_059520   \n",
       "3  SEQ_000007_000003        Target  SEQ_000007                 3  SUBJ_059520   \n",
       "4  SEQ_000007_000004        Target  SEQ_000007                 4  SUBJ_059520   \n",
       "\n",
       "                       orientation                                   behavior  \\\n",
       "0  Seated Lean Non Dom - FACE DOWN  Relaxes and moves hand to target location   \n",
       "1  Seated Lean Non Dom - FACE DOWN  Relaxes and moves hand to target location   \n",
       "2  Seated Lean Non Dom - FACE DOWN  Relaxes and moves hand to target location   \n",
       "3  Seated Lean Non Dom - FACE DOWN  Relaxes and moves hand to target location   \n",
       "4  Seated Lean Non Dom - FACE DOWN  Relaxes and moves hand to target location   \n",
       "\n",
       "        phase             gesture     acc_x  ...  tof_5_v54  tof_5_v55  \\\n",
       "0  Transition  Cheek - pinch skin  6.683594  ...       -1.0       -1.0   \n",
       "1  Transition  Cheek - pinch skin  6.949219  ...       -1.0       -1.0   \n",
       "2  Transition  Cheek - pinch skin  5.722656  ...       -1.0       -1.0   \n",
       "3  Transition  Cheek - pinch skin  6.601562  ...       -1.0       -1.0   \n",
       "4  Transition  Cheek - pinch skin  5.566406  ...       -1.0       -1.0   \n",
       "\n",
       "   tof_5_v56  tof_5_v57  tof_5_v58  tof_5_v59  tof_5_v60  tof_5_v61  \\\n",
       "0       -1.0       -1.0       -1.0       -1.0       -1.0       -1.0   \n",
       "1       -1.0       -1.0       -1.0       -1.0       -1.0       -1.0   \n",
       "2      112.0      119.0       -1.0       -1.0       -1.0       -1.0   \n",
       "3      101.0      111.0       -1.0       -1.0       -1.0       -1.0   \n",
       "4      101.0      109.0      125.0       -1.0       -1.0       -1.0   \n",
       "\n",
       "   tof_5_v62  tof_5_v63  \n",
       "0       -1.0       -1.0  \n",
       "1       -1.0       -1.0  \n",
       "2       -1.0       -1.0  \n",
       "3       -1.0       -1.0  \n",
       "4       -1.0       -1.0  \n",
       "\n",
       "[5 rows x 341 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv\", low_memory=False)\n",
    "test = pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv\", low_memory=False)\n",
    "train_demo = pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv\")\n",
    "test_demo = pd.read_csv(\"/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv\")\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f64fe1c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:55:19.604563Z",
     "iopub.status.busy": "2025-11-13T20:55:19.604203Z",
     "iopub.status.idle": "2025-11-13T20:55:21.874285Z",
     "shell.execute_reply": "2025-11-13T20:55:21.873129Z"
    },
    "papermill": {
     "duration": 2.277291,
     "end_time": "2025-11-13T20:55:21.876249",
     "exception": false,
     "start_time": "2025-11-13T20:55:19.598958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.fft import fft\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "# Using the user-provided file paths\n",
    "TRAIN_CSV = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train.csv'\n",
    "TEST_CSV = '/kaggle/input/cmi-detect-behavior-with-sensor-data/test.csv'\n",
    "DEMOG_TRAIN = '/kaggle/input/cmi-detect-behavior-with-sensor-data/train_demographics.csv'\n",
    "DEMOG_TEST = '/kaggle/input/cmi-detect-behavior-with-sensor-data/test_demographics.csv'\n",
    "\n",
    "FEATURE_OUT = '/kaggle/working/sequence_features.csv'\n",
    "RANDOM_SEED = 42\n",
    "WINDOW_SIZE = 50 # A tunable hyperparameter\n",
    "\n",
    "# Define sensor column groups\n",
    "ACC_COLS = ['acc_x', 'acc_y', 'acc_z']\n",
    "ROT_COLS = ['rot_w', 'rot_x', 'rot_y', 'rot_z']\n",
    "IMU_COLS = ACC_COLS + ROT_COLS\n",
    "THM_COLS = [f'thm_{i}' for i in range(1, 6)]\n",
    "TOF_COLS = [f'tof_{i}_v{j}' for i in range(1, 6) for j in range(0, 64)]\n",
    "\n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b127eeb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:55:21.885615Z",
     "iopub.status.busy": "2025-11-13T20:55:21.885136Z",
     "iopub.status.idle": "2025-11-13T20:55:21.895053Z",
     "shell.execute_reply": "2025-11-13T20:55:21.893989Z"
    },
    "papermill": {
     "duration": 0.016443,
     "end_time": "2025-11-13T20:55:21.896772",
     "exception": false,
     "start_time": "2025-11-13T20:55:21.880329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 1: FEATURE ENGINEERING FUNCTIONS\n",
    "# ===============================================\n",
    "\n",
    "def extract_imu_features(window_df):\n",
    "    \"\"\"Extracts Time-Domain, Frequency-Domain, and statistical IMU features.\"\"\"\n",
    "    features = {}\n",
    "    \n",
    "    # ... (other code for Time-Domain features)\n",
    "    \n",
    "    # Time-Domain and Statistical Features (CORRECTED SECTION)\n",
    "    for col in IMU_COLS:\n",
    "        col_prefix = col\n",
    "        features[f'{col_prefix}_mean'] = window_df[col].mean()\n",
    "        features[f'{col_prefix}_var'] = window_df[col].var()\n",
    "        \n",
    "        # Explicitly check sample size before calculating skew and kurtosis\n",
    "        data = window_df[col].dropna()\n",
    "        MIN_SAMPLE_SIZE = 8 # Set minimum required samples (e.g., 8 is a common threshold)\n",
    "        \n",
    "        if len(data) >= MIN_SAMPLE_SIZE:\n",
    "            features[f'{col_prefix}_skew'] = skew(data)\n",
    "            features[f'{col_prefix}_kurt'] = kurtosis(data)\n",
    "        else:\n",
    "            # If sample is too small, set to NaN (which the imputer will handle later)\n",
    "            features[f'{col_prefix}_skew'] = np.nan\n",
    "            features[f'{col_prefix}_kurt'] = np.nan\n",
    "    #Signal Magnitude Area (SMA) and Total Magnitude\n",
    "    acc_mag = np.sqrt(np.square(window_df[ACC_COLS]).sum(axis=1))\n",
    "    features['acc_SMA'] = acc_mag.sum() / WINDOW_SIZE\n",
    "    features['acc_mag_mean'] = acc_mag.mean()\n",
    "    features['acc_mag_var'] = acc_mag.var()\n",
    "\n",
    "    # Cross-Axis Correlation\n",
    "    features['acc_corr_xy'] = window_df['acc_x'].corr(window_df['acc_y'])\n",
    "    \n",
    "    # Frequency-Domain Features (FFT magnitude of first few non-DC components)\n",
    "    for col in IMU_COLS:\n",
    "        if len(window_df[col].dropna()) > 0:\n",
    "            fft_values = np.abs(fft(window_df[col].fillna(window_df[col].mean()).values))\n",
    "            for i in range(1, 6):\n",
    "                features[f'{col}_fft_mag_{i}'] = fft_values[i]\n",
    "        else:\n",
    "             for i in range(1, 6):\n",
    "                features[f'{col}_fft_mag_{i}'] = np.nan\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6162460a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:55:21.906632Z",
     "iopub.status.busy": "2025-11-13T20:55:21.905504Z",
     "iopub.status.idle": "2025-11-13T20:55:21.912445Z",
     "shell.execute_reply": "2025-11-13T20:55:21.911435Z"
    },
    "papermill": {
     "duration": 0.01348,
     "end_time": "2025-11-13T20:55:21.914080",
     "exception": false,
     "start_time": "2025-11-13T20:55:21.900600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_thermopile_features(window_df):\n",
    "    \"\"\"Extracts features from Thermopile data (thm_1 to thm_5), including gradients.\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Simple Statistical Features (Average Heat Intensity, Temporal Variance)\n",
    "    for col in THM_COLS:\n",
    "        col_prefix = col\n",
    "        features[f'{col_prefix}_mean'] = window_df[col].mean()\n",
    "        features[f'{col_prefix}_var'] = window_df[col].var()\n",
    "\n",
    "    # Thermopile Gradient Features (Difference between adjacent sensors)\n",
    "    for i in range(1, 5):\n",
    "        grad_col = f'thm_grad_{i}_{i+1}'\n",
    "        gradient = window_df[f'thm_{i}'] - window_df[f'thm_{i+1}']\n",
    "        features[f'{grad_col}_mean'] = gradient.mean()\n",
    "        features[f'{grad_col}_var'] = gradient.var()\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fde562fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:55:21.924104Z",
     "iopub.status.busy": "2025-11-13T20:55:21.923706Z",
     "iopub.status.idle": "2025-11-13T20:55:21.932381Z",
     "shell.execute_reply": "2025-11-13T20:55:21.931459Z"
    },
    "papermill": {
     "duration": 0.01559,
     "end_time": "2025-11-13T20:55:21.933899",
     "exception": false,
     "start_time": "2025-11-13T20:55:21.918309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_tof_features(window_df):\n",
    "    \"\"\"Extracts spatial and temporal features from Time-of-Flight (ToF) data.\"\"\"\n",
    "    features = {}\n",
    "\n",
    "    # Replace the 'no sensor response' value (-1) with NaN for statistics\n",
    "    tof_data = window_df[TOF_COLS].replace(-1, np.nan)\n",
    "    \n",
    "    # Overall Proximity Features\n",
    "    features['tof_min_proximity'] = tof_data.min().min() # Closest object\n",
    "    features['tof_max_proximity'] = tof_data.max().max() # Farthest object\n",
    "    \n",
    "    # Per-Sensor (5 sensors) Spatial and Temporal Features\n",
    "    for i in range(1, 6):\n",
    "        sensor_cols = [f'tof_{i}_v{j}' for j in range(0, 64)]\n",
    "        tof_sensor_data = tof_data[sensor_cols]\n",
    "        prefix = f'tof_{i}'\n",
    "\n",
    "        # 1. Spatial Statistics\n",
    "        spatial_mean_series = tof_sensor_data.mean(axis=1) # Mean distance per row\n",
    "        features[f'{prefix}_spatial_mean'] = spatial_mean_series.mean()\n",
    "        features[f'{prefix}_spatial_var'] = spatial_mean_series.var()\n",
    "        \n",
    "        # 2. Number of 'Active' Pixels (Non-negative)\n",
    "        active_pixels = (window_df[sensor_cols] != -1).sum(axis=1)\n",
    "        features[f'{prefix}_active_pixels_mean'] = active_pixels.mean()\n",
    "        features[f'{prefix}_active_pixels_var'] = active_pixels.var()\n",
    "\n",
    "        # 3. Temporal Slope / Trend\n",
    "        avg_proximity_series = spatial_mean_series.dropna()\n",
    "        if len(avg_proximity_series) >= 2:\n",
    "            # Calculate temporal slope (change in average distance over time)\n",
    "            slope = np.polyfit(np.arange(len(avg_proximity_series)), avg_proximity_series, 1)[0]\n",
    "            features[f'{prefix}_temporal_slope'] = slope\n",
    "        else:\n",
    "            features[f'{prefix}_temporal_slope'] = np.nan\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d8f747",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:55:21.943328Z",
     "iopub.status.busy": "2025-11-13T20:55:21.943004Z",
     "iopub.status.idle": "2025-11-13T20:55:21.950949Z",
     "shell.execute_reply": "2025-11-13T20:55:21.950088Z"
    },
    "papermill": {
     "duration": 0.014637,
     "end_time": "2025-11-13T20:55:21.952343",
     "exception": false,
     "start_time": "2025-11-13T20:55:21.937706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# STEP 2: DATA PIPELINE (Windowing, Extraction, Merging)\n",
    "# ===============================================\n",
    "\n",
    "def run_feature_pipeline(ts_path, demog_path, is_train=True):\n",
    "    \"\"\"Loads, processes time-series data, and merges with demographics.\"\"\"\n",
    "    print(f\"--- Running Pipeline for {'Train' if is_train else 'Test'} Data ---\")\n",
    "    \n",
    "    # Load Time-Series and Demographics\n",
    "    df_ts = pd.read_csv(ts_path)\n",
    "    df_demog = pd.read_csv(demog_path)\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Iterate through each unique sequence\n",
    "    sequence_groups = df_ts.groupby('sequence_id')\n",
    "    for seq_id, group in sequence_groups:\n",
    "        \n",
    "        # --- Windowing Strategy ---\n",
    "        # For a full sequence (Transition, Pause, Gesture), the critical event \n",
    "        # is the Gesture, which is at the end of the sequence. We use the last \n",
    "        # WINDOW_SIZE rows to capture this final motion.\n",
    "        window_df = group.tail(WINDOW_SIZE).copy()\n",
    "        \n",
    "        if len(window_df) < WINDOW_SIZE:\n",
    "            # Skip short sequences that can't form a reliable feature vector\n",
    "            continue\n",
    "            \n",
    "        # 1. Feature Extraction\n",
    "        features = {}\n",
    "        features.update(extract_imu_features(window_df))\n",
    "        features.update(extract_thermopile_features(window_df))\n",
    "        features.update(extract_tof_features(window_df))\n",
    "        \n",
    "        # 2. Add identifying and target columns\n",
    "        features['sequence_id'] = seq_id\n",
    "        features['subject'] = window_df['subject'].iloc[0]\n",
    "        if is_train:\n",
    "            # The gesture column is only present in the training set\n",
    "            features['gesture'] = window_df['gesture'].iloc[-1] \n",
    "        \n",
    "        features_list.append(features)\n",
    "        \n",
    "    df_features = pd.DataFrame(features_list)\n",
    "    \n",
    "    # 3. Merge with Demographics (Static Features Fusion)\n",
    "    df_final = pd.merge(df_features, df_demog, on='subject', how='left')\n",
    "    \n",
    "    print(f\"Generated {len(df_final)} feature vectors. Total features: {len(df_final.columns)}\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08f1b92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:55:21.961775Z",
     "iopub.status.busy": "2025-11-13T20:55:21.961456Z",
     "iopub.status.idle": "2025-11-13T20:58:58.013711Z",
     "shell.execute_reply": "2025-11-13T20:58:58.012502Z"
    },
    "papermill": {
     "duration": 216.059361,
     "end_time": "2025-11-13T20:58:58.015705",
     "exception": false,
     "start_time": "2025-11-13T20:55:21.956344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Pipeline for Train Data ---\n",
      "Generated 6819 feature vectors. Total features: 122\n",
      "--- Running Pipeline for Test Data ---\n",
      "Generated 2 feature vectors. Total features: 121\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline for both datasets\n",
    "df_train_features = run_feature_pipeline(TRAIN_CSV, DEMOG_TRAIN, is_train=True)\n",
    "df_test_features = run_feature_pipeline(TEST_CSV, DEMOG_TEST, is_train=False)\n",
    "\n",
    "# Save the features (optional, for debugging/reuse)\n",
    "df_train_features.to_csv(FEATURE_OUT.replace(\".csv\", \"_train.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22134652",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:58:58.025489Z",
     "iopub.status.busy": "2025-11-13T20:58:58.024778Z",
     "iopub.status.idle": "2025-11-13T20:59:14.520152Z",
     "shell.execute_reply": "2025-11-13T20:59:14.519005Z"
    },
    "papermill": {
     "duration": 16.502013,
     "end_time": "2025-11-13T20:59:14.521735",
     "exception": false,
     "start_time": "2025-11-13T20:58:58.019722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Model Training...\n",
      "Training Complete.\n",
      "Training Macro F1 Score (Validation Check): 1.0000\n",
      "\n",
      "Submission file created at: /kaggle/working/submission.csv\n",
      "  sequence_id              gesture\n",
      "0  SEQ_000001   Forehead - scratch\n",
      "1  SEQ_000011  Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: MODELING PIPELINE (Imputation, Scaling, Training)\n",
    "# ===============================================\n",
    "\n",
    "def train_and_evaluate_model(df_train_features, df_test_features, random_seed):\n",
    "    \"\"\"Prepares data, trains Random Forest model, and prepares submission.\"\"\"\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    # 1. Separate Features (X) and Target (y)\n",
    "    X_train = df_train_features.drop(columns=['sequence_id', 'subject', 'gesture'])\n",
    "    y_train = df_train_features['gesture']\n",
    "    \n",
    "    # Align test features to match training feature columns\n",
    "    X_test_seq_id = df_test_features['sequence_id']\n",
    "    X_test = df_test_features.drop(columns=['sequence_id', 'subject'])\n",
    "    \n",
    "    # 2. Define the Model Pipeline\n",
    "    IMPUTATION_STEP = ('imputer', SimpleImputer(strategy='median')) # Handles NaNs from feature engineering\n",
    "    \n",
    "    # Random Forest (Tree-based model, handles complexity, with class weighting)\n",
    "    rf_pipe = Pipeline([\n",
    "        IMPUTATION_STEP, \n",
    "        ('scaler', StandardScaler()), # Scaling helps with consistent feature contribution\n",
    "        ('clf', RandomForestClassifier(\n",
    "            n_estimators=500, # Increased n_estimators for better performance\n",
    "            random_state=random_seed,\n",
    "            class_weight='balanced', # Crucial for handling imbalanced BFRB/Non-BFRB classes\n",
    "            n_jobs=-1 # Use all cores\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    # --- Training and Prediction ---\n",
    "    print(\"\\nStarting Model Training...\")\n",
    "    rf_pipe.fit(X_train, y_train)\n",
    "    print(\"Training Complete.\")\n",
    "    \n",
    "    # Predict on Test Data\n",
    "    y_pred_test = rf_pipe.predict(X_test)\n",
    "    \n",
    "    # --- Evaluation (Conceptual: Needs cross-validation for real score) ---\n",
    "    # For a quick check, predict on training data:\n",
    "    y_pred_train = rf_pipe.predict(X_train)\n",
    "    macro_f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
    "    print(f\"Training Macro F1 Score (Validation Check): {macro_f1_train:.4f}\")\n",
    "    \n",
    "    # --- Submission File Generation ---\n",
    "    df_submission = pd.DataFrame({\n",
    "        'sequence_id': X_test_seq_id,\n",
    "        'gesture': y_pred_test\n",
    "    })\n",
    "    \n",
    "    # Final Output file path for Kaggle submission\n",
    "    submission_path = os.path.join('/kaggle/working', 'submission.csv')\n",
    "    df_submission.to_csv(submission_path, index=False)\n",
    "    print(f\"\\nSubmission file created at: {submission_path}\")\n",
    "    print(df_submission.head())\n",
    "\n",
    "# Execute the final modeling step\n",
    "train_and_evaluate_model(df_train_features, df_test_features, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6846f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T20:59:14.531860Z",
     "iopub.status.busy": "2025-11-13T20:59:14.531526Z",
     "iopub.status.idle": "2025-11-13T21:00:13.919236Z",
     "shell.execute_reply": "2025-11-13T21:00:13.916686Z"
    },
    "papermill": {
     "duration": 59.396562,
     "end_time": "2025-11-13T21:00:13.922489",
     "exception": false,
     "start_time": "2025-11-13T20:59:14.525927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Model Development and Evaluation...\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Macro F1 Score (Validation): 0.5029\n",
      "\n",
      "Training k-Nearest Neighbors...\n",
      "k-Nearest Neighbors Macro F1 Score (Validation): 0.4049\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree Macro F1 Score (Validation): 0.4203\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Macro F1 Score (Validation): 0.5629\n",
      "\n",
      "Training Voting Classifier...\n",
      "Voting Classifier Macro F1 Score (Validation): 0.4526\n",
      "\n",
      "Retraining BEST model (Random Forest) on full training data...\n",
      "\n",
      "--- Summary of Validation F1 Scores ---\n",
      "Logistic Regression: 0.5029\n",
      "k-Nearest Neighbors: 0.4049\n",
      "Decision Tree: 0.4203\n",
      "Random Forest: 0.5629\n",
      "Voting Classifier: 0.4526\n",
      "\n",
      "Final prediction made using: Random Forest\n",
      "Submission file created at: /kaggle/working/submission_ensemble.csv\n",
      "  sequence_id              gesture\n",
      "0  SEQ_000001   Forehead - scratch\n",
      "1  SEQ_000011  Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier # NEW\n",
    "from sklearn.neighbors import KNeighborsClassifier # NEW\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier # UPDATED\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import os\n",
    "\n",
    "# --- Imbalance Handling Note ---\n",
    "# Class Weighting (class_weight='balanced') is used for LR, DT, and RF as it's built-in \n",
    "# and often preferred inside pipelines. SMOTE, though effective, requires an external \n",
    "# library (imblearn) and must be applied BEFORE scaling, which complicates the pipeline.\n",
    "\n",
    "def train_and_evaluate_model(df_train_features, df_test_features, random_seed):\n",
    "    \"\"\"Defines, trains, and evaluates all baseline, tree-based, and ensemble models.\"\"\"\n",
    "    \n",
    "    print(\"\\nStarting Model Development and Evaluation...\")\n",
    "    \n",
    "    # --- Data Preparation ---\n",
    "    X_train_full = df_train_features.drop(columns=['sequence_id', 'subject', 'gesture'])\n",
    "    y_train_full = df_train_features['gesture']\n",
    "    \n",
    "    # Use a split of the TRAIN data for internal evaluation (Macro F1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.3, random_state=random_seed, stratify=y_train_full\n",
    "    )\n",
    "    \n",
    "    # Prepare Test Data for final predictions\n",
    "    X_test_seq_id = df_test_features['sequence_id']\n",
    "    X_test = df_test_features.drop(columns=['sequence_id', 'subject'])\n",
    "    \n",
    "    # --- Pipeline Components ---\n",
    "    IMPUTATION_STEP = ('imputer', SimpleImputer(strategy='median'))\n",
    "    SCALING_STEP = ('scaler', StandardScaler())\n",
    "    \n",
    "    \n",
    "    # --- 1. Baseline Models ---\n",
    "    \n",
    "    # 1A. Logistic Regression (Imbalanced handled by class_weight)\n",
    "    lr_pipe = Pipeline([\n",
    "        IMPUTATION_STEP, SCALING_STEP,\n",
    "        ('clf', LogisticRegression(random_state=random_seed, solver='liblinear', multi_class='ovr', class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # 1B. k-Nearest Neighbors (No inherent imbalance handling, scaling is crucial)\n",
    "    knn_pipe = Pipeline([\n",
    "        IMPUTATION_STEP, SCALING_STEP,\n",
    "        ('clf', KNeighborsClassifier(n_neighbors=5)) # n_neighbors is tunable\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # --- 2. Tree-based Models ---\n",
    "    \n",
    "    # 2A. Decision Tree (Imbalanced handled by class_weight)\n",
    "    dt_pipe = Pipeline([\n",
    "        IMPUTATION_STEP,\n",
    "        ('clf', DecisionTreeClassifier(random_state=random_seed, class_weight='balanced'))\n",
    "    ])\n",
    "    \n",
    "    # 2B. Random Forest (Imbalanced handled by class_weight)\n",
    "    rf_pipe = Pipeline([\n",
    "        IMPUTATION_STEP,\n",
    "        ('clf', RandomForestClassifier(n_estimators=500, random_state=random_seed, class_weight='balanced', n_jobs=-1))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    # --- 3. Ensemble Model: Voting Classifier ---\n",
    "    \n",
    "    # Use the best performing and most diverse base models: LR, RF, and DT.\n",
    "    # The 'named_estimators' must use the name of the step defined above.\n",
    "    \n",
    "    \n",
    "    # Wrap the ensemble in a pipeline for consistent data preprocessing\n",
    "    voting_pipe = VotingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', lr_pipe),\n",
    "            ('rf', rf_pipe),\n",
    "            ('dt', dt_pipe)\n",
    "        ],\n",
    "        voting='soft', \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # --- 4. Training and Evaluation Loop ---\n",
    "    \n",
    "    models = {\n",
    "        \"Logistic Regression\": lr_pipe,\n",
    "        \"k-Nearest Neighbors\": knn_pipe,\n",
    "        \"Decision Tree\": dt_pipe,\n",
    "        \"Random Forest\": rf_pipe,\n",
    "        \"Voting Classifier\": voting_pipe\n",
    "    }\n",
    "    \n",
    "    best_model_name = \"\"\n",
    "    best_f1 = -1.0\n",
    "    \n",
    "    results = {}\n",
    "    for name, pipe in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        pipe.fit(X_train, y_train)\n",
    "        y_pred_val = pipe.predict(X_val)\n",
    "        \n",
    "        macro_f1 = f1_score(y_val, y_pred_val, average='macro')\n",
    "        results[name] = macro_f1\n",
    "        print(f\"{name} Macro F1 Score (Validation): {macro_f1:.4f}\")\n",
    "\n",
    "        if macro_f1 > best_f1:\n",
    "            best_f1 = macro_f1\n",
    "            best_model_name = name\n",
    "            \n",
    "    # --- 5. Final Prediction using the BEST model ---\n",
    "    \n",
    "    final_model = models[best_model_name]\n",
    "    \n",
    "    # Retrain the best model on the FULL training set for best performance\n",
    "    print(f\"\\nRetraining BEST model ({best_model_name}) on full training data...\")\n",
    "    final_model.fit(X_train_full, y_train_full)\n",
    "    \n",
    "    y_pred_test = final_model.predict(X_test)\n",
    "    \n",
    "    # --- Submission File Generation ---\n",
    "    df_submission = pd.DataFrame({\n",
    "        'sequence_id': X_test_seq_id,\n",
    "        'gesture': y_pred_test\n",
    "    })\n",
    "    \n",
    "    submission_path = os.path.join('/kaggle/working', 'submission_ensemble.csv')\n",
    "    df_submission.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(\"\\n--- Summary of Validation F1 Scores ---\")\n",
    "    for name, f1 in results.items():\n",
    "         print(f\"{name}: {f1:.4f}\")\n",
    "         \n",
    "    print(f\"\\nFinal prediction made using: {best_model_name}\")\n",
    "    print(f\"Submission file created at: {submission_path}\")\n",
    "    print(df_submission.head())\n",
    "    \n",
    "# Execute the final modeling step\n",
    "train_and_evaluate_model(df_train_features, df_test_features, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a2426b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T21:00:13.951944Z",
     "iopub.status.busy": "2025-11-13T21:00:13.951252Z",
     "iopub.status.idle": "2025-11-13T21:00:13.979841Z",
     "shell.execute_reply": "2025-11-13T21:00:13.977163Z"
    },
    "papermill": {
     "duration": 0.052785,
     "end_time": "2025-11-13T21:00:13.985796",
     "exception": false,
     "start_time": "2025-11-13T21:00:13.933011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dtype map to save memory:\n",
    "# Most sensor data can be safely cast to float32.\n",
    "DTYPE_MAP = {\n",
    "    'row_id': 'object',\n",
    "    'sequence_id': 'object',\n",
    "    'sequence_counter': 'int16',\n",
    "    'subject': 'object',\n",
    "    # IMU & THM: Use float32\n",
    "    **{col: 'float32' for col in ACC_COLS + ROT_COLS + THM_COLS},\n",
    "    # ToF: They are uncalibrated sensor values (0-254) but loaded as float/int. \n",
    "    # Use float32 to be safe with operations.\n",
    "    **{col: 'float32' for col in TOF_COLS}, \n",
    "    # Target columns (for train only)\n",
    "    'sequence_type': 'object', \n",
    "    'gesture': 'object', \n",
    "    'orientation': 'object',\n",
    "    'behavior': 'object'\n",
    "}\n",
    "\n",
    "\n",
    "def run_feature_pipeline_optimized(ts_path, demog_path, is_train=True):\n",
    "    \"\"\"Loads data with optimized dtypes, processes, and merges with demographics.\"\"\"\n",
    "    print(f\"--- Running Optimized Pipeline for {'Train' if is_train else 'Test'} Data ---\")\n",
    "    \n",
    "    # Load Time-Series with optimized dtypes\n",
    "    # We load only the columns we need to save even more memory\n",
    "    cols_to_load = ['sequence_id', 'subject', 'sequence_counter'] + IMU_COLS + THM_COLS + TOF_COLS\n",
    "    if is_train:\n",
    "         cols_to_load += ['gesture']\n",
    "    \n",
    "    # Filter DTYPE_MAP to only include columns being loaded\n",
    "    dtype_filter = {k: v for k, v in DTYPE_MAP.items() if k in cols_to_load}\n",
    "    \n",
    "    # Load the time-series file with explicit dtypes\n",
    "    df_ts = pd.read_csv(ts_path, dtype=dtype_filter, usecols=cols_to_load)\n",
    "    df_demog = pd.read_csv(demog_path)\n",
    "    \n",
    "    # ... (rest of the run_feature_pipeline function remains the same)\n",
    "    \n",
    "    features_list = []\n",
    "    \n",
    "    # Iterate through each unique sequence\n",
    "    sequence_groups = df_ts.groupby('sequence_id')\n",
    "    for seq_id, group in sequence_groups:\n",
    "        # ... (feature extraction code remains the same)\n",
    "        window_df = group.tail(WINDOW_SIZE).copy()\n",
    "        \n",
    "        if len(window_df) < WINDOW_SIZE:\n",
    "            continue\n",
    "            \n",
    "        features = {}\n",
    "        features.update(extract_imu_features(window_df))\n",
    "        features.update(extract_thermopile_features(window_df))\n",
    "        features.update(extract_tof_features(window_df))\n",
    "        \n",
    "        features['sequence_id'] = seq_id\n",
    "        features['subject'] = window_df['subject'].iloc[0]\n",
    "        if is_train:\n",
    "            features['gesture'] = window_df['gesture'].iloc[-1] \n",
    "        \n",
    "        features_list.append(features)\n",
    "        \n",
    "    df_features = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Crucial Memory Step: Explicitly delete large dataframe after features are extracted\n",
    "    del df_ts\n",
    "    \n",
    "    # Merge with Demographics\n",
    "    df_final = pd.merge(df_features, df_demog, on='subject', how='left')\n",
    "    \n",
    "    print(f\"Generated {len(df_final)} feature vectors. Total features: {len(df_final.columns)}\")\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "286f1eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T21:00:14.017325Z",
     "iopub.status.busy": "2025-11-13T21:00:14.016669Z",
     "iopub.status.idle": "2025-11-13T21:05:06.759315Z",
     "shell.execute_reply": "2025-11-13T21:05:06.758108Z"
    },
    "papermill": {
     "duration": 292.760065,
     "end_time": "2025-11-13T21:05:06.761136",
     "exception": false,
     "start_time": "2025-11-13T21:00:14.001071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Optimized Pipeline for Train Data ---\n",
      "Generated 6819 feature vectors. Total features: 122\n",
      "--- Running Optimized Pipeline for Test Data ---\n",
      "Generated 2 feature vectors. Total features: 121\n",
      "\n",
      "Starting Model Development and Evaluation...\n",
      "\n",
      "Training Logistic Regression...\n",
      "Logistic Regression Macro F1 Score (Validation): 0.5029\n",
      "\n",
      "Training k-Nearest Neighbors...\n",
      "k-Nearest Neighbors Macro F1 Score (Validation): 0.4049\n",
      "\n",
      "Training Decision Tree...\n",
      "Decision Tree Macro F1 Score (Validation): 0.4203\n",
      "\n",
      "Training Random Forest...\n",
      "Random Forest Macro F1 Score (Validation): 0.5632\n",
      "\n",
      "Training Voting Classifier...\n",
      "Voting Classifier Macro F1 Score (Validation): 0.4526\n",
      "\n",
      "Retraining BEST model (Random Forest) on full training data...\n",
      "\n",
      "--- Summary of Validation F1 Scores ---\n",
      "Logistic Regression: 0.5029\n",
      "k-Nearest Neighbors: 0.4049\n",
      "Decision Tree: 0.4203\n",
      "Random Forest: 0.5632\n",
      "Voting Classifier: 0.4526\n",
      "\n",
      "Final prediction made using: Random Forest\n",
      "Submission file created at: /kaggle/working/submission_ensemble.csv\n",
      "  sequence_id              gesture\n",
      "0  SEQ_000001   Forehead - scratch\n",
      "1  SEQ_000011  Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline for both datasets using the optimized function\n",
    "df_train_features = run_feature_pipeline_optimized(TRAIN_CSV, DEMOG_TRAIN, is_train=True)\n",
    "df_test_features = run_feature_pipeline_optimized(TEST_CSV, DEMOG_TEST, is_train=False)\n",
    "\n",
    "# Save the features\n",
    "# Saving to disk ensures the memory is cleared and the feature set is usable even if later steps fail.\n",
    "df_train_features.to_csv(FEATURE_OUT.replace(\".csv\", \"_train.csv\"), index=False)\n",
    "df_test_features.to_csv(FEATURE_OUT.replace(\".csv\", \"_test.csv\"), index=False)\n",
    "\n",
    "# Execute the final modeling step (assuming train_and_evaluate_model is defined correctly)\n",
    "train_and_evaluate_model(df_train_features, df_test_features, RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "084a3bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-13T21:05:06.773136Z",
     "iopub.status.busy": "2025-11-13T21:05:06.772774Z",
     "iopub.status.idle": "2025-11-13T21:05:06.781168Z",
     "shell.execute_reply": "2025-11-13T21:05:06.780039Z"
    },
    "papermill": {
     "duration": 0.016399,
     "end_time": "2025-11-13T21:05:06.782677",
     "exception": false,
     "start_time": "2025-11-13T21:05:06.766278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  sequence_id              gesture\n",
      "0  SEQ_000001   Forehead - scratch\n",
      "1  SEQ_000011  Eyelash - pull hair\n"
     ]
    }
   ],
   "source": [
    "h=pd.read_csv('/kaggle/working/submission.csv')\n",
    "print(h)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12518947,
     "sourceId": 102335,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 678.387588,
   "end_time": "2025-11-13T21:05:09.711281",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-13T20:53:51.323693",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
